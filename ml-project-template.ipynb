{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TITLE] General Data Science/ML Project Template\n",
    "\n",
    "- Author: Kevin Chuang [@k-chuang](https://github.com/k-chuang)\n",
    "- Date: 10/07/2018\n",
    "- Description: A jupyter notebook template for steps in solving a data science and/or machine learning problem.\n",
    "- Dataset: [Link to dataset source]()\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- **Introduction / Abstract**\n",
    "- **Load libraries & get data**\n",
    "    - Split data to training and test set\n",
    "        - stratified sampling based on certain feature(s) or label(s)\n",
    "- **Exploratory Data Analysis**\n",
    "    - Discover and visualize the training data to gain insights\n",
    "- **Data Preprocessing**\n",
    "    - Prepare data for ML algorithms\n",
    "    - Write pipelines using transformers to do automated feature engineering:\n",
    "        - Scale data\n",
    "        - Impute missing data (or remove)\n",
    "        - Feature creation\n",
    "- **Model Selection & Training**\n",
    "    - Use K-Folds Cross-Validation to select top 2 to 5 most promising models\n",
    "        - Do not spend too much time tweaking hyperparameters\n",
    "    - Typical ML models include kNN, SVM, linear/logistic regression, ensemble methods (RF, XGB), neural networks, etc.\n",
    "    - [Optional] Save experimental models to pickle file.\n",
    "- **Model Tuning**\n",
    "    - `GridSearchCV`, `RandomSearchCV`, or `BayesSearchCV`\n",
    "        - `GridSearchCV`: brute force way to search for 'best' hyperparameters\n",
    "        - `BayesSearchCV`: smart way to use Bayesian inference to optimally search for best hyperparameters\n",
    "- **Model Evaluation**\n",
    "    - Final evaluation on hold out test set\n",
    "    - If regression, calculate 95% confidence interval range\n",
    "        - t score or z score to calculate confidence interval\n",
    "- **Solution Presentation and/or submission**\n",
    "    - What I learned, what worked & what did not, what assumptions were made, and what system's limitations are\n",
    "    - Create clear visualizations & easy-to-remember statements\n",
    "- **Deployment**\n",
    "    - Clean up and concatenate pipleines to single pipeline to do full data preparation plus final prediction\n",
    "    - Create programs to monitor & check system's live performance    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction / Abstract\n",
    "\n",
    "- Write a paragraph about the project/problem at hand\n",
    "    - Look at the big picture\n",
    "    - Frame the problem\n",
    "        - Business objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries & data\n",
    "\n",
    "- Load important libraries\n",
    "- Load (or acquire) associated data\n",
    "- Split data into training and test set\n",
    "    - Based on either feature importance or class imbalance, use *stratified sampling* to split data to keep porportion even for training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Kevin Chuang (https://www.github.com/k-chuang)' \n",
    "\n",
    "# linear algebra\n",
    "import numpy as np \n",
    "\n",
    "# data processing\n",
    "import pandas as pd \n",
    "\n",
    "# data visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Algorithms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Metrics \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Model Selection & Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space  import Real, Categorical, Integer\n",
    "\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Mathematical Functions\n",
    "import math\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "- Visualize training data using different kinds of plots\n",
    "- Plot dependent variables (features) against independent variable (target label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- Writing pipelines to do automated feature engineering\n",
    "    - Imputing missing values (or removing values)\n",
    "    - Scaling data\n",
    "    - Transforming objects (strings, dates, etc.) to numerical vectors\n",
    "    - Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection & Training\n",
    "\n",
    "- Try different models and choose best 2-5 models\n",
    "    - Use K-Fold cross-validation to validate which models are the best\n",
    "- Typical ML models include kNN, SVM, linear/logistic regression, ensemble methods (RF, XGB), neural networks, etc.\n",
    "- [Optional] Save experimental models to pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "- Tune the top chosen model(s) and tune hyperparameters\n",
    "    - Ideally, use Bayes Optimization `BayesSearchCV` to optimally search for best hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "- Final evaluation on the test set\n",
    "- Calculation of confidence intervals using t-score or z-scores to give a range of values and confidence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
